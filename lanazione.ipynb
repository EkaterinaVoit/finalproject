{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2227.1 Safari/537.36'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from pprint import pprint\n",
    "from fake_useragent import UserAgent\n",
    "\n",
    "session = requests.session()\n",
    "\n",
    "response = session.get('https://www.lanazione.it/cronaca?page=1')\n",
    "\n",
    "ua = UserAgent(verify_ssl=False)\n",
    "\n",
    "headers = {'User-Agent': ua.random}\n",
    "#print(headers)\n",
    "response = session.get('https://www.lanazione.it/cronaca?page=1', headers=headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обкачаем немного новостей с сайта.\n",
    "\n",
    "1. Страницы имеют вид \"https://www.lanazione.it/cronaca?page={page_number}\", поэтому можно просто идти циклом.\n",
    "2. Достанем заголовок, краткое описание (из станицы со списком новостей), текст полной статьи (из самой страницы новости)\n",
    "3. Положим в базу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('la_nazione_news.db')\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создаем базу данных, где будем хранить информацию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('la_nazione_news.db')\n",
    "cur = conn.cursor()\n",
    "cur.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS texts \n",
    "(id INTEGER PRIMARY KEY AUTOINCREMENT, news_id text, title text, short_text text, full_text text)\n",
    "\"\"\")\n",
    "\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Шаг 1. Найти страницы**\n",
    "\n",
    "Посмотрим, как устроены новости и скачаем одну страницу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Arezzo, 30 marzo 2022 -\\xa0Dimenticare i rimpianti del derby per sfruttare la seconda chance casalinga e agganciare i maremmani al terzo posto. Ha più di un motivo l’Arezzo per battere il Tiferno Lerchi nel turno infrasettimanale. \\nMariotti inserisce Van Der Velden al posto dello squalificato Biondi. A centrocampo tornano dall'inizio Marchi e Pizzutelli, completa il reparto Mancino. Attacco inedito con Doratiotto dietro Cutolo e Calderini.\\xa0L'allenatore degli ospiti Machi schiera\\xa0i suoi con il colla\""
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page_number = 1\n",
    "url = f'https://www.lanazione.it/cronaca?page={page_number}'\n",
    "req = session.get(url, headers={'User-Agent': ua.random})\n",
    "page = req.text\n",
    "page = requests.get(url)\n",
    "soup = BeautifulSoup(page.text, 'html.parser')\n",
    "#print(soup.prettify())\n",
    "\n",
    "#отдельные посты\n",
    "news = soup.find_all('div', {'class': 'card__text'}) # news = soup.find_all('header', {'class': 'card__title'})\n",
    "#len(news)\n",
    "\n",
    "#Заголовок-ссылка и текст заголовка\n",
    "title_obj = news[0].find('a', {'class': 'title__link'})\n",
    "#title_obj\n",
    "title = title_obj.text\n",
    "#title\n",
    "\n",
    "#достаем ссылку\n",
    "attrs = title_obj.attrs\n",
    "#attrs\n",
    "href = title_obj.attrs['href']\n",
    "#href\n",
    "\n",
    "#короткий текст новости\n",
    "short_text =  news[0].find('div', {'class': 'abstract'}).text\n",
    "#short_text\n",
    "\n",
    "#парсим страничку одной новости\n",
    "url_one = href\n",
    "#url_one\n",
    "\n",
    "req = session.get(url_one, headers={'User-Agent': ua.random})\n",
    "page_n = req.text\n",
    "page_n = requests.get(url_one)\n",
    "\n",
    "soup = BeautifulSoup(page_n.text, 'html.parser')\n",
    "#print(soup.prettify())\n",
    "\n",
    "#сохраняем текст статьи\n",
    "full_text = soup.find('article', {'class': 'sc-1ig42x7-6 iSFXhF'}).text\n",
    "#full_text[:500]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Здесь я попыталась лемматизировать только текст одной новости** (но что-то пошло не так)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_texts = full_text\n",
    "#articles_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_preprocessed = []\n",
    "for a_text in articles_texts:\n",
    "    a_tokens = wordpunct_tokenize(a_text)\n",
    "    a_lemmatized = \" \".join([morph.parse(item)[0].normal_form for item in a_tokens])\n",
    "    articles_preprocessed.append(a_lemmatized)\n",
    "print (articles_preprocessed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Здесь я оформила ранее ивлеченные данные в формулы**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "#функция для парсинга страницы со списокм новостей (блок одной новости)\n",
    "def parse_news_page_block(one_block):\n",
    "    block = {}\n",
    "    a = one_block.find('a', {'class': 'title__link'})\n",
    "    block['title'] = a.text\n",
    "    block['href'] = a.attrs['href']\n",
    "    block['short_text'] = one_block.find('div', {'class': 'abstract'}).text\n",
    "    return block\n",
    "\n",
    "page_number = 1\n",
    "url = f'https://www.lanazione.it/cronaca?page={page_number}'\n",
    "req = session.get(url, headers={'User-Agent': ua.random})\n",
    "page = req.text\n",
    "page = requests.get(url)\n",
    "soup = BeautifulSoup(page.text, 'html.parser')\n",
    "news = soup.find_all('div', {'class': 'card__text'})\n",
    "# for n in news: \n",
    "#     print(parse_news_page_block(n))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': \"Trovato morto tra la differenziata: l'autopsia, decesso per compressione\", 'href': 'https://www.lanazione.it/empoli/cronaca/morto-empoli-carta-1.7519532', 'short_text': \"\\n                    Empoli, la vicenda dell'extracomunitario rinvenuto cadavere in un'azienda che tratta la carta: il giovane si sarebbe addormentato nel contenitore\\n            \", 'full_text': \"Empoli, 30 marzo 2022 - Si avvalora l'ipotesi che la morte del giovane somalo trovato in un contenitore della differenziata sia morto per una tragica fatalità. Ovvero che si sia addormentato in un cassonetto salvo poi essere inconsapevolmente portato via con il camion dagli addetti. L'autopsia infatti rivela, nelle sue prime risultanze, che il ragazzo è morto a causa di una compressione.\\xa0Il cadavere è stato rinvenuto da un operaio in una ditta cartaria di Empoli che stava proprio smistanto il materiale cartaceo arrivato dalla raccolta differenziata. Operando con la gru ha visto il corpo. Comprensibile lo choc degli operai che hanno subito dato l'allarme.Per il giovane, originario della Somalia, arrivato in Italia per cercare un futuro migliore, non c'è stato niente da fare. Il contenitore arrivava con certezza dalla zona di Firenze, come è stato ricostruito.Si è indagato in tutte le direzioni ma l'autopsia svolta sul corpo adesso avvalora l'ipotesi appunto della fatalità. Il giovane ha trovato rifugio nel cassonetto, poi è stato portato via con il contenitore dai mezzi addetti alla raccolta finendo per essere colpito dai macchinari.\\xa0Sulla vicenda la procura ha aperto un fascicolo per omicidio colposo, senza indagati. Sembra allontanarsi l'ipotesi dunque dell'omicidio o di un tentativo di disfarsi del corpo da parte di qualcuno.\\xa0\\n\\n\\n\\n\"}\n"
     ]
    }
   ],
   "source": [
    "#функция для парсинга отдельной страницы новости\n",
    "def parse_one_article(block):\n",
    "    url_one = block['href']\n",
    "    req = session.get(url_one, headers={'User-Agent': ua.random})\n",
    "    page = req.text\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "    block['full_text'] = soup.find('article', {'class': 'sc-1ig42x7-6 iSFXhF'}).text\n",
    "    return block\n",
    "\n",
    "page_number = 1\n",
    "url = f'https://www.lanazione.it/cronaca?page={page_number}'\n",
    "req = session.get(url, headers={'User-Agent': ua.random})\n",
    "page = req.text\n",
    "page = requests.get(url)\n",
    "soup = BeautifulSoup(page.text, 'html.parser')\n",
    "news = soup.find_all('div', {'class': 'card__text'})\n",
    "print(parse_one_article(parse_news_page_block(news[1])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Здесь я попыталась лемматизировать постранично, но безуспешно**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from pymorphy2 import MorphAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "morph = MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "#функция для парсинга отдельной страницы новости\n",
    "def parse_one_article(block):\n",
    "    url_one = block['href']\n",
    "    req = session.get(url_one, headers={'User-Agent': ua.random})\n",
    "    page = req.text\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "    block['full_text'] = soup.find('article', {'class': 'sc-1ig42x7-6 iSFXhF'}).text\n",
    "    return block\n",
    "\n",
    "page_number = 1\n",
    "url = f'https://www.lanazione.it/cronaca?page={page_number}'\n",
    "req = session.get(url, headers={'User-Agent': ua.random})\n",
    "page = req.text\n",
    "page = requests.get(url)\n",
    "soup = BeautifulSoup(page.text, 'html.parser')\n",
    "news = soup.find_all('div', {'class': 'card__text'})\n",
    "#print(parse_one_article(parse_news_page_block(news[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_texts = parse_one_article(parse_news_page_block(news[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['title', 'href', 'short_text', 'full_text']\n"
     ]
    }
   ],
   "source": [
    "articles_preprocessed = []\n",
    "for a_text in articles_texts:\n",
    "    a_tokens = wordpunct_tokenize(a_text)\n",
    "    a_lemmatized = \" \".join([morph.parse(item)[0].normal_form for item in a_tokens])\n",
    "    articles_preprocessed.append(a_lemmatized)\n",
    "print (articles_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tfidf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/Air11/Downloads/Новая папка/hackathon/sampleproject/lanazione.ipynb Cell 19'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/Air11/Downloads/%D0%9D%D0%BE%D0%B2%D0%B0%D1%8F%20%D0%BF%D0%B0%D0%BF%D0%BA%D0%B0/hackathon/sampleproject/lanazione.ipynb#ch0000094?line=0'>1</a>\u001b[0m articles_tfidf \u001b[39m=\u001b[39m tfidf\u001b[39m.\u001b[39mfit_transform(articles_preprocessed)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/Air11/Downloads/%D0%9D%D0%BE%D0%B2%D0%B0%D1%8F%20%D0%BF%D0%B0%D0%BF%D0%BA%D0%B0/hackathon/sampleproject/lanazione.ipynb#ch0000094?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mМатрица на \u001b[39m\u001b[39m{\u001b[39;00marticles_tfidf\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m документов и \u001b[39m\u001b[39m{\u001b[39;00marticles_tfidf\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m термов\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tfidf' is not defined"
     ]
    }
   ],
   "source": [
    "articles_tfidf = tfidf.fit_transform(articles_preprocessed)\n",
    "print(f\"Матрица на {articles_tfidf.shape[0]} документов и {articles_tfidf.shape[1]} термов\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Регулярное выражение для того, чтобы достать ID новости и не повторяться"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_news_id = re.compile('/([0-9]*?).html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обработать N-ую страницу новостей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "seen_news = []\n",
    "exceptions = []\n",
    "def get_nth_page(page_number):\n",
    "    # скачиваем\n",
    "    url = f'https://www.lanazione.it/cronaca?page={page_number}'\n",
    "    req = session.get(url, headers={'User-Agent': ua.random})\n",
    "    page = req.text\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "    \n",
    "    # находим новости\n",
    "    news = soup.find_all('div', {'class': 'card__text'})\n",
    "    \n",
    "    # идем по новостям и обрабатываем их\n",
    "    blocks = []\n",
    "    for n in news:\n",
    "        try:\n",
    "            blocks.append(parse_news_page_block(n))\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "    # идем по отдельным статьям и достаем информацию\n",
    "    result = []\n",
    "    for b in blocks:\n",
    "        if b['href'].startswith('/'):\n",
    "            idx = regex_news_id.findall(b['href'])[0]\n",
    "            if idx not in seen_news:\n",
    "                try:\n",
    "                    res = parse_one_article(b)\n",
    "                    res['news_id'] = idx\n",
    "                    result.append(res)\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "            else:\n",
    "                print('Seen', b['href'])\n",
    "    \n",
    "    # возвращаем найденную информацию\n",
    "    return result\n",
    "\n",
    "# print(get_nth_page(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Здесь я попыталась записать в базу данных данные, которые я ранее оформила в функции**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('la_nazione_news1.db')\n",
    "cur = conn.cursor()\n",
    "cur.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS texts \n",
    "(id INTEGER PRIMARY KEY AUTOINCREMENT, news_id text, title text, short_text text, full_text text)\n",
    "\"\"\")\n",
    "\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_db(block):\n",
    "\n",
    "    # сохраняем информацию по текстам\n",
    "    cur.execute(\n",
    "        \"\"\"\n",
    "        INSERT INTO texts \n",
    "            (news_id, title, short_text, full_text) \n",
    "            VALUES (?, ?, ?, ?)\n",
    "        \"\"\", (\n",
    "            block['news_id'],\n",
    "            block['title'], block['short_text'], block['full_text'])\n",
    "    )\n",
    "    \n",
    "    # достаем id текста\n",
    "    cur.execute(\"SELECT id FROM texts WHERE news_id = ?\", (block['news_id'],))\n",
    "    text_id = cur.fetchone()[0]\n",
    "    \n",
    "    conn.commit()\n",
    "    \n",
    "    # добавляем, что такой id уже видели\n",
    "    seen_news.add(block['news_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('la_nazione_news1.db')\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# достаем, какие новости уже видели (если потом захотим обновлять базу)\n",
    "cur.execute('SELECT news_id FROM texts')\n",
    "seen_news = set(i[0] for i in cur.fetchall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем функцию, куда передаем количество страниц и она выполняет все нужные действия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all(n_pages):\n",
    "    for i in tqdm(range(n_pages)):\n",
    "        blocks = get_nth_page(i+1)\n",
    "        for block in blocks:\n",
    "            write_to_db(block)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запускаем на 20 первых страниц"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.40s/it]\n"
     ]
    }
   ],
   "source": [
    "run_all(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = stopwords.words(\"italian\")\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "    analyzer=\"word\", # анализировать по словам или по символам (char)\n",
    "    stop_words=stops # передаём список стоп-слов для русского из NLTK\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from pymorphy2 import MorphAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "morph = MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('la_nazione_news1.db')\n",
    "cur = conn.cursor()\n",
    "cur.execute(\"\"\"\n",
    "SELECT full_text FROM texts\n",
    "\"\"\")\n",
    "all_texts = cur.fetchall()\n",
    "#print(all_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_preprocessed = []\n",
    "punct = '.,?!»«№/#%*()—-[\"]@'\n",
    "for text in all_texts:\n",
    "    for a_text in text:\n",
    "        a_tokens = wordpunct_tokenize(a_text)\n",
    "        for item in a_tokens:\n",
    "            if item not in punct:\n",
    "                a_lemmatized = \" \".join([morph.parse(item)[0].normal_form])\n",
    "                texts_preprocessed.append(a_lemmatized)\n",
    "#print(texts_preprocessed)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "69a01369cc2b5228cafb9ffed76b5ca0cff0e98a4f79195f600e5d8040b6f0a7"
  },
  "kernelspec": {
   "display_name": "projectname",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
